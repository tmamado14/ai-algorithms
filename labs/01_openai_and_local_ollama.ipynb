{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a386164e",
   "metadata": {},
   "source": [
    "# Lab 01: LLM API Fundamentals and Multi-turn Context (Ollama + OpenAI)\n",
    "\n",
    "This lab is local-first for zero API cost in class.\n",
    "1. Local model access via Ollama (primary)\n",
    "2. OpenAI API usage (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Before running:\n",
    "- Install dependencies with `uv sync`\n",
    "- Create `.env` from `.env.example`\n",
    "- Ensure Ollama is running and your Qwen model is installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-nano\")\n",
    "\n",
    "def has_real_openai_key(value: str | None) -> bool:\n",
    "    if not value or not value.strip():\n",
    "        return False\n",
    "    cleaned = value.strip()\n",
    "    lowered = cleaned.lower()\n",
    "    if lowered in {\"your_openai_api_key_here\", \"sk-your_key_here\"}:\n",
    "        return False\n",
    "    if lowered.startswith(\"your_\"):\n",
    "        return False\n",
    "    return cleaned.startswith(\"sk-\")\n",
    "\n",
    "OPENAI_ENABLED = has_real_openai_key(OPENAI_API_KEY)\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"qwen3:8b\")\n",
    "\n",
    "print(\"OPENAI_MODEL (optional):\", OPENAI_MODEL)\n",
    "print(\"OPENAI enabled:\", OPENAI_ENABLED)\n",
    "print(\"OLLAMA_BASE_URL:\", OLLAMA_BASE_URL)\n",
    "print(\"OLLAMA_MODEL:\", OLLAMA_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How `chat.completions.create` Works\n",
    "\n",
    "- `model`: the model name (for example `qwen3:8b` on Ollama or `gpt-5-nano` on OpenAI).\n",
    "- `messages`: conversation history passed to the model.\n",
    "- `temperature`: controls randomness (lower = more deterministic).\n",
    "\n",
    "### Message Roles\n",
    "- `system`: global behavior/instructions for the assistant.\n",
    "- `user`: the current user request.\n",
    "- `assistant`: previous model output included as conversation context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Local Ollama (Primary): OpenAI-Compatible Qwen Call\n",
    "\n",
    "Use this section as the default classroom path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_BASE_URL.rstrip('/')}/v1\",\n",
    "    api_key=os.getenv(\"OLLAMA_API_KEY\", \"ollama\"),\n",
    ")\n",
    "\n",
    "ollama_response = ollama_client.chat.completions.create(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what a token is in LLMs in 2 sentences.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## Qwen Response\\n\\n\" + ollama_response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c71f3",
   "metadata": {},
   "source": [
    "## 2) OpenAI API (Optional)\n",
    "\n",
    "Use this only if you want to spend on hosted/premium models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_ENABLED:\n",
    "    display(Markdown(\"**Skipped:** set a real `OPENAI_API_KEY` in `.env` to run this section.\"))\n",
    "else:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    openai_response = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain what a token is in LLMs in 2 sentences.\"}\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "    display(Markdown(\"## OpenAI Response\\n\\n\" + openai_response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643066d6",
   "metadata": {},
   "source": [
    "## 3) Compare Responses\n",
    "\n",
    "Use the same prompt and compare local vs cloud output quality, style, and latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_prompt = \"Give 3 practical tips to write better prompts for beginner LLM users.\"\n",
    "\n",
    "ollama_compare = ollama_client.chat.completions.create(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": shared_prompt}],\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## Ollama (Qwen)\\n\\n\" + ollama_compare.choices[0].message.content))\n",
    "\n",
    "if OPENAI_ENABLED:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    openai_compare = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": shared_prompt}],\n",
    "        temperature=1,\n",
    "    )\n",
    "    display(Markdown(\"## OpenAI\\n\\n\" + openai_compare.choices[0].message.content))\n",
    "else:\n",
    "    display(Markdown(\"**OpenAI compare skipped** because `OPENAI_API_KEY` is not configured.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01c25f",
   "metadata": {},
   "source": [
    "## 4) Multi-turn Example: Feed Assistant Output Back As Context\n",
    "\n",
    "This demonstrates how an assistant message from turn 1 is sent back in turn 2 using `role=\"assistant\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic = \"vector databases in RAG\"\n",
    "\n",
    "turn1 = ollama_client.chat.completions.create(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful instructor.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate one strong interview question about {seed_topic}. Return the question only\"},\n",
    "    ],\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "assistant_question = turn1.choices[0].message.content\n",
    "display(Markdown(\"## Turn 1: Assistant-Generated Question\\n\\n\" + assistant_question))\n",
    "\n",
    "turn2 = ollama_client.chat.completions.create(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful instructor.\"},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_question},\n",
    "        {\"role\": \"user\", \"content\": \"Now answer that question in a concise beginner-friendly way.\"},\n",
    "    ],\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## Turn 2: Answer Using Assistant Context\\n\\n\" + turn2.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6190144",
   "metadata": {},
   "source": [
    "## 5) Multi-turn Example (OpenAI API, Optional)\n",
    "\n",
    "Same pattern as above, but using OpenAI API. This is optional and may incur cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPENAI_ENABLED:\n",
    "    display(Markdown(\"**Skipped:** set a real `OPENAI_API_KEY` in `.env` to run this section.\"))\n",
    "else:\n",
    "    seed_topic = \"function calling in agents\"\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    openai_turn1 = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful instructor.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate one strong interview question about {seed_topic}. Return the question only\"},\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    openai_assistant_question = openai_turn1.choices[0].message.content\n",
    "    display(Markdown(\"## OpenAI Turn 1: Assistant-Generated Question\\n\\n\" + openai_assistant_question))\n",
    "\n",
    "    openai_turn2 = openai_client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful instructor.\"},\n",
    "            {\"role\": \"assistant\", \"content\": openai_assistant_question},\n",
    "            {\"role\": \"user\", \"content\": \"Now answer that question in 4 short bullet points for beginners.\"},\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    display(Markdown(\"## OpenAI Turn 2: Answer Using Assistant Context\\n\\n\" + openai_turn2.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Exercises\n",
    "- Build your own 3-turn loop: (1) model generates a question, (2) you pass that as an `assistant` message, (3) model answers and then proposes one follow-up question.\n",
    "- Repeat the loop for 2 more turns while keeping the full `messages` history.\n",
    "- Compare how the conversation quality changes when you remove prior `assistant` messages.\n",
    "- Optional: run the same loop on both Ollama and OpenAI and compare results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd4ae8",
   "metadata": {},
   "source": [
    "**Hint (persistent message history):** keep a single `messages` list, append each new `assistant` output and `user` prompt, then pass the full list again in the next call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2f436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

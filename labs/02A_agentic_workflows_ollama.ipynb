{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab 02A: Agentic Workflow Patterns (Ollama Implementation)\n",
    "\n",
    "This lab implements the five agentic workflow patterns described in Anthropic's engineering blog post [*Building Effective Agents*](https://www.anthropic.com/engineering/building-effective-agents).\n",
    "\n",
    "All examples run **locally via Ollama** (zero API cost).\n",
    "\n",
    "---\n",
    "\n",
    "### Patterns covered\n",
    "\n",
    "| # | Pattern | Core idea |\n",
    "|---|---------|----------|\n",
    "| 1 | **Prompt Chaining** | Sequential steps where each LLM call feeds the next |\n",
    "| 2 | **Routing** | Classify input â†’ dispatch to a specialised prompt/model |\n",
    "| 3 | **Parallelization** | Run multiple LLM calls simultaneously, then aggregate |\n",
    "| 4 | **Orchestrator-Workers** | Central LLM breaks down a task and delegates to workers |\n",
    "| 5 | **Evaluator-Optimizer** | Generate â†’ Evaluate â†’ Refine loop |\n",
    "\n",
    "> **Key insight from Anthropic:** Start simple. Only add complexity when it demonstrably improves outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Same environment as Lab 01. Make sure Ollama is running and your model is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL    = os.getenv(\"OLLAMA_MODEL\", \"qwen3:8b\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_BASE_URL.rstrip('/')}/v1\",\n",
    "    api_key=os.getenv(\"OLLAMA_API_KEY\", \"ollama\"),\n",
    ")\n",
    "\n",
    "def chat(messages: list[dict], temperature: float = 0.7) -> str:\n",
    "    \"\"\"Thin wrapper: returns the assistant reply as a plain string.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Model:\", OLLAMA_MODEL)\n",
    "print(\"Ready âœ“\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chain_theory",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern 1 â€” Prompt Chaining\n",
    "\n",
    "**Concept:** Decompose a complex task into a fixed sequence of smaller LLM calls. Each step's output becomes the next step's input. A *gate* (programmatic check) can be inserted between steps to catch problems early.\n",
    "\n",
    "```\n",
    "User Input â†’ [Step 1 LLM] â†’ output_1 â†’ (gate?) â†’ [Step 2 LLM] â†’ output_2 â†’ â€¦\n",
    "```\n",
    "\n",
    "**Use when:** the task can be cleanly decomposed into sequential subtasks, and you want to trade latency for higher accuracy per step.\n",
    "\n",
    "**Example below:** Blog post pipeline â€” outline â†’ gate check â†’ full draft â†’ polish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chain_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"Why vector databases matter for RAG applications\"\n",
    "\n",
    "# â”€â”€ Step 1: Generate an outline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "outline = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a technical content strategist.\"},\n",
    "    {\"role\": \"user\",   \"content\": f\"Write a 3-section outline for a blog post about: {TOPIC}. Return only the outline.\"},\n",
    "])\n",
    "display(Markdown(\"### Step 1 â€” Outline\\n\\n\" + outline))\n",
    "\n",
    "# â”€â”€ Gate: check the outline has exactly 3 sections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "section_count = outline.count(\"\\n#\") + outline.count(\"\\n1.\") + outline.count(\"\\n2.\")\n",
    "gate_pass = len(outline.strip()) > 50   # simple length guard\n",
    "print(f\"\\nğŸ” Gate check â€” outline non-empty: {gate_pass}\")\n",
    "assert gate_pass, \"Outline too short â€” halting pipeline.\"\n",
    "\n",
    "# â”€â”€ Step 2: Write the draft â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "draft = chat([\n",
    "    {\"role\": \"system\",    \"content\": \"You are a senior technical writer.\"},\n",
    "    {\"role\": \"user\",      \"content\": f\"Topic: {TOPIC}\"},\n",
    "    {\"role\": \"assistant\", \"content\": outline},\n",
    "    {\"role\": \"user\",      \"content\": \"Expand the outline into a concise 200-word blog post draft.\"},\n",
    "])\n",
    "display(Markdown(\"### Step 2 â€” Draft\\n\\n\" + draft))\n",
    "\n",
    "# â”€â”€ Step 3: Polish for a non-technical audience â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "polished = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are an editor who simplifies technical writing.\"},\n",
    "    {\"role\": \"user\",   \"content\": f\"Rewrite the following blog post so it is engaging for a non-technical audience. Keep it under 150 words.\\n\\n{draft}\"},\n",
    "])\n",
    "display(Markdown(\"### Step 3 â€” Polished\\n\\n\" + polished))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routing_theory",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern 2 â€” Routing\n",
    "\n",
    "**Concept:** A *classifier* LLM (or rule) reads the input and assigns it a category. A second LLM call â€” with a prompt specialised for that category â€” handles the actual response.\n",
    "\n",
    "```\n",
    "User Input â†’ [Classifier LLM] â†’ category â†’ [Specialist LLM for category] â†’ Response\n",
    "```\n",
    "\n",
    "**Use when:** you have distinct input types that benefit from different prompts, tools, or even different models.\n",
    "\n",
    "**Example below:** Customer support triage â€” billing, technical, or general queries each get a tailored response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALIST_PROMPTS = {\n",
    "    \"billing\":   \"You are a billing support specialist. Be empathetic and solution-focused.\",\n",
    "    \"technical\": \"You are a senior technical support engineer. Give precise, step-by-step answers.\",\n",
    "    \"general\":   \"You are a friendly customer success agent. Be warm and concise.\",\n",
    "}\n",
    "\n",
    "def classify_query(user_query: str) -> str:\n",
    "    \"\"\"Return one of: billing | technical | general\"\"\"\n",
    "    result = chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"Classify the following customer query into exactly one category: \"\n",
    "                \"billing, technical, or general. \"\n",
    "                \"Reply with the single word only.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    ).strip().lower()\n",
    "    # Normalise â€” model might add punctuation\n",
    "    for key in SPECIALIST_PROMPTS:\n",
    "        if key in result:\n",
    "            return key\n",
    "    return \"general\"\n",
    "\n",
    "def routed_response(user_query: str) -> str:\n",
    "    category = classify_query(user_query)\n",
    "    system_prompt = SPECIALIST_PROMPTS[category]\n",
    "    answer = chat([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_query},\n",
    "    ])\n",
    "    return category, answer\n",
    "\n",
    "# â”€â”€ Test three query types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "queries = [\n",
    "    \"I was charged twice on my last invoice.\",\n",
    "    \"My API requests keep returning a 429 error even though I'm below the limit.\",\n",
    "    \"What are your business hours?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    category, answer = routed_response(q)\n",
    "    display(Markdown(f\"**Query:** {q}\\n\\n**Routed to:** `{category}`\\n\\n**Response:** {answer}\\n\\n---\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel_theory",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern 3 â€” Parallelization\n",
    "\n",
    "**Concept:** Run multiple independent LLM calls at the same time and aggregate the results. Two sub-patterns:\n",
    "\n",
    "- **Sectioning** â€” split a task into independent chunks (e.g. evaluate different aspects of code).\n",
    "- **Voting** â€” run the same prompt N times and take a majority decision.\n",
    "\n",
    "```\n",
    "               â”Œâ”€ [Worker 1] â”€â”\n",
    "Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€ [Worker 2] â”€â”œâ”€â”€â–º Aggregator â”€â”€â–º Final output\n",
    "               â””â”€ [Worker 3] â”€â”˜\n",
    "```\n",
    "\n",
    "**Use when:** subtasks are independent, or you need higher confidence via multiple perspectives.\n",
    "\n",
    "**Example below (Sectioning):** Evaluate a code snippet on three independent axes simultaneously â€” correctness, readability, security.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_SNIPPET = \"\"\"\n",
    "def get_user(user_id):\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
    "    return db.execute(query).fetchone()\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_AXES = {\n",
    "    \"correctness\":  \"Review the code only for logical correctness and potential bugs.\",\n",
    "    \"readability\":  \"Review the code only for readability, naming, and documentation.\",\n",
    "    \"security\":     \"Review the code only for security vulnerabilities.\",\n",
    "}\n",
    "\n",
    "def review_axis(axis: str, instructions: str) -> tuple[str, str]:\n",
    "    result = chat([\n",
    "        {\"role\": \"system\", \"content\": instructions + \" Be concise (3â€“5 sentences).\"},\n",
    "        {\"role\": \"user\",   \"content\": f\"```python\\n{CODE_SNIPPET}\\n```\"},\n",
    "    ])\n",
    "    return axis, result\n",
    "\n",
    "# â”€â”€ Run reviews in parallel using threads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with ThreadPoolExecutor(max_workers=3) as pool:\n",
    "    futures = {pool.submit(review_axis, axis, instr): axis\n",
    "               for axis, instr in REVIEW_AXES.items()}\n",
    "    reviews = {}\n",
    "    for future in futures:\n",
    "        axis, result = future.result()\n",
    "        reviews[axis] = result\n",
    "\n",
    "# â”€â”€ Aggregation step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "review_text = \"\\n\\n\".join(f\"### {k.title()}\\n{v}\" for k, v in reviews.items())\n",
    "display(Markdown(\"## Parallel Code Reviews\\n\\n\" + review_text))\n",
    "\n",
    "summary = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a lead code reviewer. Summarise the findings below into a single priority-ordered action list.\"},\n",
    "    {\"role\": \"user\",   \"content\": review_text},\n",
    "])\n",
    "display(Markdown(\"## Aggregated Action List\\n\\n\" + summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel_voting_theory",
   "metadata": {},
   "source": [
    "### Parallelization â€” Voting variant\n",
    "\n",
    "Run the same classification N times and take a majority vote. Useful when a single call may be unreliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel_voting_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "REVIEW_TEXT = \"\"\"\n",
    "The new product update is amazing! I love how smooth everything feels now.\n",
    "Although the onboarding is still a bit confusing, the core experience is 10/10.\n",
    "\"\"\"\n",
    "\n",
    "def classify_sentiment(_: int) -> str:\n",
    "    \"\"\"Classify sentiment as positive, negative, or mixed.\"\"\"\n",
    "    return chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"Classify the sentiment as exactly one word: positive, negative, or mixed.\"},\n",
    "            {\"role\": \"user\",   \"content\": REVIEW_TEXT},\n",
    "        ],\n",
    "        temperature=1,   # higher temp â†’ more variance across runs\n",
    "    ).strip().lower().split()[0]\n",
    "\n",
    "N_VOTES = 5\n",
    "with ThreadPoolExecutor(max_workers=N_VOTES) as pool:\n",
    "    votes = list(pool.map(classify_sentiment, range(N_VOTES)))\n",
    "\n",
    "tally = Counter(votes)\n",
    "winner = tally.most_common(1)[0][0]\n",
    "display(Markdown(f\"**Votes:** {dict(tally)}\\n\\n**Majority verdict:** `{winner}`\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orchestrator_theory",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern 4 â€” Orchestrator-Workers\n",
    "\n",
    "**Concept:** A central *orchestrator* LLM receives the goal, breaks it into subtasks dynamically, and delegates each subtask to a *worker* LLM. The orchestrator then synthesises all worker outputs.\n",
    "\n",
    "```\n",
    "Goal â”€â”€â–º [Orchestrator LLM] â”€â”€â–º subtask_1 â”€â”€â–º [Worker 1]\n",
    "                             â”œâ”€â–º subtask_2 â”€â”€â–º [Worker 2]   â”€â”€â–º [Orchestrator synthesises]\n",
    "                             â””â”€â–º subtask_3 â”€â”€â–º [Worker 3]\n",
    "```\n",
    "\n",
    "**Key difference from Parallelization:** subtasks are *not pre-defined*; the orchestrator decides them at runtime.\n",
    "\n",
    "**Use when:** the task is complex and the required subtasks aren't known in advance.\n",
    "\n",
    "**Example below:** Research assistant â€” the orchestrator decides which sub-questions to investigate, workers answer each, orchestrator synthesises a final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orchestrator_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_GOAL = \"Explain how Retrieval-Augmented Generation (RAG) works and when to use it.\"\n",
    "\n",
    "# â”€â”€ Orchestrator: break into subtasks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plan_raw = chat(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are a research orchestrator. Given a goal, output a JSON array of 3â€“4 \"\n",
    "            \"focused sub-questions to investigate. Return ONLY valid JSON, no other text.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": RESEARCH_GOAL},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Parse JSON (strip markdown fences if the model adds them)\n",
    "plan_clean = plan_raw.strip().removeprefix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").strip()\n",
    "subtasks: list[str] = json.loads(plan_clean)\n",
    "display(Markdown(\"### Orchestrator Plan\\n\\n\" + \"\\n\".join(f\"{i+1}. {t}\" for i, t in enumerate(subtasks))))\n",
    "\n",
    "# â”€â”€ Workers: answer each subtask in parallel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def worker_answer(question: str) -> tuple[str, str]:\n",
    "    answer = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise AI/ML expert. Answer in 3â€“5 sentences.\"},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ])\n",
    "    return question, answer\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(subtasks)) as pool:\n",
    "    worker_results = dict(pool.map(lambda q: worker_answer(q), subtasks))\n",
    "\n",
    "worker_text = \"\\n\\n\".join(f\"**Q: {q}**\\n\\n{a}\" for q, a in worker_results.items())\n",
    "display(Markdown(\"### Worker Answers\\n\\n\" + worker_text))\n",
    "\n",
    "# â”€â”€ Orchestrator: synthesise into a final report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "final_report = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a senior technical writer. Synthesise the Q&A pairs below into a coherent, structured mini-report of ~200 words.\"},\n",
    "    {\"role\": \"user\",   \"content\": f\"Goal: {RESEARCH_GOAL}\\n\\n{worker_text}\"},\n",
    "])\n",
    "display(Markdown(\"### Final Synthesised Report\\n\\n\" + final_report))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluator_theory",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern 5 â€” Evaluator-Optimizer\n",
    "\n",
    "**Concept:** A *generator* LLM produces output; an *evaluator* LLM scores it and provides feedback; the generator uses that feedback to improve. The loop continues until quality criteria are met or a maximum iteration count is reached.\n",
    "\n",
    "```\n",
    "Task â”€â”€â–º [Generator] â”€â”€â–º draft\n",
    "                â–²              â”‚\n",
    "           feedback      [Evaluator]\n",
    "                â–²              â”‚\n",
    "                â””â”€â”€â”€â”€ score â—„â”€â”€â”˜\n",
    "           (loop until score â‰¥ threshold or max_iters)\n",
    "```\n",
    "\n",
    "**Use when:** you have clear quality criteria and iterative refinement measurably improves output. Analogous to a human writer â†’ editor cycle.\n",
    "\n",
    "**Example below:** Refine a technical explanation until the evaluator scores it â‰¥ 8/10 for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluator_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITING_TASK = \"Explain how attention mechanisms work in transformer models, for a junior developer.\"\n",
    "SCORE_THRESHOLD = 8\n",
    "MAX_ITERATIONS  = 4\n",
    "\n",
    "def generate(task: str, feedback: str | None = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a technical educator. Write clearly and concisely (â‰¤150 words).\"},\n",
    "        {\"role\": \"user\",   \"content\": task},\n",
    "    ]\n",
    "    if feedback:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Improve your previous response based on this feedback:\\n{feedback}\"})\n",
    "    return chat(messages)\n",
    "\n",
    "def evaluate(task: str, response: str) -> tuple[int, str]:\n",
    "    \"\"\"Return (score 1-10, actionable feedback).\"\"\"\n",
    "    raw = chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a strict writing evaluator. \"\n",
    "                \"Score the response from 1â€“10 for clarity and accuracy given the task. \"\n",
    "                \"Reply ONLY with JSON: {\\\"score\\\": <int>, \\\"feedback\\\": \\\"<string>\\\"}\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"Task: {task}\\n\\nResponse:\\n{response}\"},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    ).strip()\n",
    "    clean = raw.removeprefix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").strip()\n",
    "    parsed = json.loads(clean)\n",
    "    return int(parsed[\"score\"]), parsed[\"feedback\"]\n",
    "\n",
    "# â”€â”€ Optimization loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feedback = None\n",
    "for iteration in range(1, MAX_ITERATIONS + 1):\n",
    "    draft = generate(WRITING_TASK, feedback)\n",
    "    score, feedback = evaluate(WRITING_TASK, draft)\n",
    "\n",
    "    display(Markdown(\n",
    "        f\"### Iteration {iteration} â€” Score: {score}/10\\n\\n\"\n",
    "        f\"**Draft:**\\n\\n{draft}\\n\\n\"\n",
    "        f\"**Evaluator feedback:** {feedback}\"\n",
    "    ))\n",
    "\n",
    "    if score >= SCORE_THRESHOLD:\n",
    "        display(Markdown(f\"âœ… **Threshold reached at iteration {iteration}. Final answer above.**\"))\n",
    "        break\n",
    "else:\n",
    "    display(Markdown(f\"âš ï¸ Max iterations reached. Best score: {score}/10\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_section",
   "metadata": {},
   "source": [
    "---\n",
    "## Pattern Comparison\n",
    "\n",
    "Run the same task through **Prompt Chaining** vs **Evaluator-Optimizer** and compare quality vs latency trade-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TASK = \"Write a 100-word explanation of embeddings for a product manager.\"\n",
    "\n",
    "# â”€â”€ Prompt Chaining: outline â†’ draft (2 steps, no feedback loop) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "outline_pm = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a technical content strategist.\"},\n",
    "    {\"role\": \"user\",   \"content\": f\"Write a 3-point outline for: {TASK}\"},\n",
    "])\n",
    "draft_pm = chat([\n",
    "    {\"role\": \"system\",    \"content\": \"You are a technical writer for business audiences.\"},\n",
    "    {\"role\": \"assistant\", \"content\": outline_pm},\n",
    "    {\"role\": \"user\",      \"content\": \"Expand into a 100-word explanation.\"},\n",
    "])\n",
    "chain_time = time.time() - t0\n",
    "\n",
    "# â”€â”€ Evaluator-Optimizer: 1 generation + 1 refinement pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "draft_eo = generate(TASK)\n",
    "score_eo, fb_eo = evaluate(TASK, draft_eo)\n",
    "if score_eo < 8:\n",
    "    draft_eo = generate(TASK, fb_eo)\n",
    "eo_time = time.time() - t0\n",
    "\n",
    "display(Markdown(\n",
    "    f\"## Prompt Chaining result ({chain_time:.1f}s)\\n\\n{draft_pm}\\n\\n---\\n\\n\"\n",
    "    f\"## Evaluator-Optimizer result ({eo_time:.1f}s) â€” initial score {score_eo}/10\\n\\n{draft_eo}\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises_section",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "Work through as many as you like. Start with (1) and go deeper as time permits.\n",
    "\n",
    "1. **Prompt Chaining - add retry + structured gate:** In the blog pipeline (Pattern 1), enforce a strict outline format (exactly 3 numbered sections with titles only). If the gate fails, automatically re-prompt the model up to 2 retries before halting. Print which attempt passed.\n",
    "\n",
    "2. **Routing â€” add a fourth category:** Extend the routing example to handle `\"feature_request\"` queries. Write a specialist prompt and test it with at least two example inputs.\n",
    "\n",
    "3. **Parallelization â€” build a guardrail:** Create a parallel two-worker setup where Worker A generates a response to a user question while Worker B simultaneously checks whether the question is safe/appropriate. Combine the two results â€” only return Worker A's output if Worker B gives the green light.\n",
    "\n",
    "4. **Orchestrator-Workers â€” dynamic depth:** Modify the orchestrator example so that after the first round of worker answers, the orchestrator decides whether a second round of deeper sub-questions is warranted. Implement at most 2 rounds.\n",
    "\n",
    "5. **Evaluator-Optimizer â€” custom rubric:** Change the evaluator prompt to score on three separate axes (clarity, accuracy, brevity) and return a JSON object with all three scores. Stop the loop only when *all three* scores are â‰¥ 7.\n",
    "\n",
    "6. **(Stretch) Combine patterns:** Build a mini content pipeline that uses:\n",
    "   - **Routing** to detect if the topic is technical or business-oriented,\n",
    "   - **Orchestrator-Workers** to research the topic in parallel, and\n",
    "   - **Evaluator-Optimizer** to refine the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercises_scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
